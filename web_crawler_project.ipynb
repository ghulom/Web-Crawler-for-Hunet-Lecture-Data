{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2f3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "This is a web crawler for collecting lecture data from the Hunet site.\n",
      "====================================================================================================\n",
      "1. What keyword would you like to collect data for? (e.g., Python): Python\n",
      "2. How many lectures would you like to collect? (default: 10): 20\n",
      "3. Enter the folder path to save files (default: c:\\py_temp\\): /Users/ghulom97/Desktop/gooo\n",
      "No more pages to load or error occurred: Alert Text: 마지막 페이지입니다.\n",
      "Message: unexpected alert open: {Alert text : 마지막 페이지입니다.}\n",
      "  (Session info: chrome=127.0.6533.89)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000104925088 cxxbridge1$str$ptr + 1887276\n",
      "1   chromedriver                        0x000000010491d764 cxxbridge1$str$ptr + 1856264\n",
      "2   chromedriver                        0x000000010452c82c cxxbridge1$string$len + 88524\n",
      "3   chromedriver                        0x00000001045a89e8 cxxbridge1$string$len + 596872\n",
      "4   chromedriver                        0x0000000104565474 cxxbridge1$string$len + 321044\n",
      "5   chromedriver                        0x00000001045660e4 cxxbridge1$string$len + 324228\n",
      "6   chromedriver                        0x00000001048eca6c cxxbridge1$str$ptr + 1656336\n",
      "7   chromedriver                        0x00000001048f14c8 cxxbridge1$str$ptr + 1675372\n",
      "8   chromedriver                        0x00000001048d2950 cxxbridge1$str$ptr + 1549556\n",
      "9   chromedriver                        0x00000001048f1c78 cxxbridge1$str$ptr + 1677340\n",
      "10  chromedriver                        0x00000001048c4660 cxxbridge1$str$ptr + 1491460\n",
      "11  chromedriver                        0x000000010490eac0 cxxbridge1$str$ptr + 1795684\n",
      "12  chromedriver                        0x000000010490ec3c cxxbridge1$str$ptr + 1796064\n",
      "13  chromedriver                        0x000000010491d398 cxxbridge1$str$ptr + 1855292\n",
      "14  libsystem_pthread.dylib             0x000000018b82df94 _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000018b828d34 thread_start + 8\n",
      "\n",
      "Total 11 lectures found.\n",
      "Collecting images and lecture details...\n",
      "1 image saved.\n",
      "2 image saved.\n",
      "3 image saved.\n",
      "4 image saved.\n",
      "5 image saved.\n",
      "6 image saved.\n",
      "7 image saved.\n",
      "8 image saved.\n",
      "9 image saved.\n",
      "10 image saved.\n",
      "11 image saved.\n",
      "1. Title: Python을 활용한 빅데이터 분석과 시각화, Satisfaction: 4.9, Reviews: 5, Price: 110,000\n",
      "2. Title: 칼퇴를 위한 데이터 분석 정복코스 (Feat.Python), Satisfaction: 4.9, Reviews: 0, Price: 522,500\n",
      "3. Title: 비전공자도 쉽게 하는Python입문, Satisfaction: 4.9, Reviews: 1, Price: 110,000\n",
      "4. Title: 코알못 탈출! 왕초보 파이썬 수업 [초급], Satisfaction: 4.7, Reviews: 0, Price: 200,000\n",
      "5. Title: [K-디지털 기초역량훈련] 실습으로 뽀개는 왕초보 파이썬, Satisfaction: 4.3, Reviews: 19, Price: 200,000\n",
      "6. Title: [ITⓔ코칭] 파이썬으로 시작하는 친절한 코딩클래스, Satisfaction: 4.5, Reviews: 4, Price: 85,000\n",
      "7. Title: 파이썬을 이용한 자동화 스크립트, Satisfaction: 4.8, Reviews: 17, Price: 159,000\n",
      "8. Title: Power BI로 배우는 데이터 시각화, Satisfaction: 4.3, Reviews: 7, Price: 100,000\n",
      "9. Title: 쉽고 빠르게 배우는 파이썬 GUI 프로그래밍, Satisfaction: 4.5, Reviews: 28, Price: 130,000\n",
      "10. Title: 떠먹여주는 ChatGPT 데이터 분석(feat. 파이썬), Satisfaction: 4.9, Reviews: 14, Price: 130,000\n",
      "11. Title: 나의 첫 웹크롤링 & 텍스트마이닝 with 파이썬, Satisfaction: 4.9, Reviews: 0, Price: 110,000\n",
      "CSV file saved at: /Users/ghulom97/Desktop/gooo/2024-08-03-23-08-22-Python-hunet_lecture/2024-08-03-23-08-22-Python-hunet_lecture.csv\n",
      "Excel file saved at: /Users/ghulom97/Desktop/gooo/2024-08-03-23-08-22-Python-hunet_lecture/2024-08-03-23-08-22-Python-hunet_lecture.xlsx\n",
      "Excel file with images saved at: /Users/ghulom97/Desktop/gooo/2024-08-03-23-08-22-Python-hunet_lecture/2024-08-03-23-08-22-Python-hunet_lecture.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Step 1: Gather input from the user\n",
    "print(\"=\" * 100)\n",
    "print(\"This is a web crawler for collecting lecture data from the Hunet site.\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "query_txt = input('1. What keyword would you like to collect data for? (e.g., Python): ')\n",
    "\n",
    "try:\n",
    "    cnt = int(input('2. How many lectures would you like to collect? (default: 10): '))\n",
    "except ValueError:\n",
    "    cnt = 10\n",
    "    print('Proceeding with the default value of 10 lectures.')\n",
    "\n",
    "# Calculate the number of pages to scrape based on the number of lectures\n",
    "page_cnt = math.ceil(cnt / 12)\n",
    "\n",
    "f_dir = input('3. Enter the folder path to save files (default: c:\\\\py_temp\\\\): ')\n",
    "if not f_dir:\n",
    "    f_dir = 'c:\\\\py_temp\\\\'\n",
    "\n",
    "# Step 2: Create a folder to save the results\n",
    "n = time.localtime()\n",
    "d = f'{n.tm_year:04d}-{n.tm_mon:02d}-{n.tm_mday:02d}-{n.tm_hour:02d}-{n.tm_min:02d}-{n.tm_sec:02d}'\n",
    "\n",
    "sec_name = 'hunet_lecture'\n",
    "save_path = os.path.join(f_dir, f'{d}-{query_txt}-{sec_name}')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "img_dir = os.path.join(save_path, 'images')\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "# Step 3: Set up the Chrome WebDriver and open the Hunet website\n",
    "s = Service(\"/Users/ghulom97/Downloads/chromedriver-mac-arm64/chromedriver\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://www.hunet.co.kr/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 4: Search for the keyword\n",
    "search_box = driver.find_element(By.ID, 'txtKeyword')\n",
    "search_box.click()\n",
    "search_box.send_keys(query_txt)\n",
    "search_box.send_keys(\"\\n\")\n",
    "\n",
    "# Step 5: Click 'Load More' to get the desired number of lectures\n",
    "for _ in range(page_cnt):\n",
    "    try:\n",
    "        load_more_button = driver.find_element(By.XPATH, '//*[@id=\"divEducationList\"]/div/div[2]/div[5]/a[1]')\n",
    "        load_more_button.click()\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(\"No more pages to load or error occurred:\", e)\n",
    "        break\n",
    "\n",
    "# Step 6: Extract lecture data\n",
    "lecture_data = {\n",
    "    'Number': [],\n",
    "    'Title': [],\n",
    "    'Satisfaction': [],\n",
    "    'Reviews': [],\n",
    "    'Price': []\n",
    "}\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "lectures = soup.find('ul', class_='vod_list').find_all('li')\n",
    "\n",
    "print(f'Total {len(lectures)} lectures found.')\n",
    "print('Collecting images and lecture details...')\n",
    "\n",
    "img_urls = [img['src'] for img in soup.find('ul', class_='vod_list').find_all('img')]\n",
    "for idx, img_url in enumerate(img_urls):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(img_url, os.path.join(img_dir, f'{idx+1}.jpg'))\n",
    "        print(f'{idx+1} image saved.')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image {idx+1}: {e}\")\n",
    "\n",
    "for idx, lecture in enumerate(lectures, start=1):\n",
    "    title = lecture.find('div', class_='title').get_text(strip=True)\n",
    "    satisfaction = lecture.find('span', class_='ex').find('strong').get_text()\n",
    "    reviews = lecture.find('span', class_='review').find('strong').get_text()\n",
    "    price = lecture.find('span', class_='expense').find('strong').get_text()\n",
    "\n",
    "    lecture_data['Number'].append(idx)\n",
    "    lecture_data['Title'].append(title)\n",
    "    lecture_data['Satisfaction'].append(satisfaction)\n",
    "    lecture_data['Reviews'].append(reviews)\n",
    "    lecture_data['Price'].append(price)\n",
    "\n",
    "    print(f'{idx}. Title: {title}, Satisfaction: {satisfaction}, Reviews: {reviews}, Price: {price}')\n",
    "\n",
    "# Step 7: Save the data to CSV and Excel files\n",
    "lecture_df = pd.DataFrame(lecture_data)\n",
    "\n",
    "csv_path = os.path.join(save_path, f'{d}-{query_txt}-{sec_name}.csv')\n",
    "lecture_df.to_csv(csv_path, encoding='utf-8-sig', index=False)\n",
    "print(f\"CSV file saved at: {csv_path}\")\n",
    "\n",
    "excel_path = os.path.join(save_path, f'{d}-{query_txt}-{sec_name}.xlsx')\n",
    "lecture_df.to_excel(excel_path, index=False)\n",
    "print(f\"Excel file saved at: {excel_path}\")\n",
    "\n",
    "# Adding images to Excel (Optional)\n",
    "workbook = Workbook()\n",
    "sheet = workbook.active\n",
    "\n",
    "# Add headers\n",
    "headers = ['Number', 'Title', 'Satisfaction', 'Reviews', 'Price', 'Image']\n",
    "sheet.append(headers)\n",
    "\n",
    "# Add lecture data and images to the Excel sheet\n",
    "for idx, (number, title, satisfaction, reviews, price) in enumerate(zip(\n",
    "    lecture_data['Number'], \n",
    "    lecture_data['Title'], \n",
    "    lecture_data['Satisfaction'], \n",
    "    lecture_data['Reviews'], \n",
    "    lecture_data['Price']\n",
    "), start=1):\n",
    "    # Add lecture data to the sheet\n",
    "    sheet.append([number, title, satisfaction, reviews, price])\n",
    "    \n",
    "    # Add image if it exists\n",
    "    img_path = os.path.join(img_dir, f'{number}.jpg')\n",
    "    if os.path.exists(img_path):\n",
    "        img = ExcelImage(img_path)\n",
    "        img.width = 130\n",
    "        img.height = 100\n",
    "        sheet.add_image(img, f'F{idx+1}')  # Adding image in the 6th column\n",
    "\n",
    "# Save the workbook with images\n",
    "workbook.save(excel_path)\n",
    "print(f\"Excel file with images saved at: {excel_path}\")\n",
    "\n",
    "# Clean up\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c1ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
